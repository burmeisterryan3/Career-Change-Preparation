{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "With linear regression, we attempted to predict a quantitative response variable using a set of quantitative explanatory or predictor variables. With logistic regression, we will use our explanatory variables to predict between one of two possible outcomes.\n",
    "\n",
    "Some examples of logistic regression use cases include determining whether a transaction is fraudulent or not, whether a user will click on a website link/resource or not, and whether an individual will default on a loan or not.\n",
    "\n",
    "NOTE: When using $log$ below, assume base $e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations\n",
    "\n",
    "With linear regression, our outputs were not constrained. Any output was possible for a given input. With logistic regression, we will constrain our outputs between 0 and 1 to reflect the probability for one of our outcomes. For instance, if we are attempting to predict if a transaction is fraudulent, we can set the value of our response variable to 1 for all ground truth data with fraudulent transactions.\n",
    "\n",
    "If we make $p$ the probability of category 1 occurring $\\left( p = P\\{Y=1\\}\\right)$, then we have\n",
    "$$logit(p)=log\\left(\\frac{p}{1-p}\\right)=b_0+b_1x_1+b_2x_2+\\dots$$\n",
    "\n",
    "* $\\frac{p}{1-p}$ is known as the *odds ratio* and is the ratio of an event occurring vs not occurring.\n",
    "* The $log$ ensures our predictions are between 0 and 1.\n",
    "* $logit(p)$ is a common shorthand when using logistic regression.\n",
    "\n",
    "Using algebra, we can see that\n",
    "$${p = \\frac{e^{b_0+b_1x_1+b_2x_2+\\dots}}{1+e^{b_0+b_1x_1+b_2x_2+\\dots}}=\\frac{1}{1+e^{-(b_0+b_1x_1+b_2x_2+\\dots)}},}$$\n",
    "and this is known as the *sigmoid function*.\n",
    "\n",
    "See [Example 1](Examples\\Regression\\logistic-regression.ipynb) for an example using fraudulent charges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results - [Helpful Link](https://towardsdatascience.com/a-simple-interpretation-of-logistic-regression-coefficients-e3a40a62e8cf)\n",
    "\n",
    "##### Defining *odds*\n",
    "\n",
    "Remember, that odds and probabilities are not the same. As a simple example, the *probability* of getting a 4 when rolling a fair, six-sided die is $\\frac{1}{6}\\approx 16.7\\%$. However, the *odds* represent the ratio of the probability of success to the probability of failure. So, in the case of getting a 4 from a fair, six-sided die, the odds are equal to $\\frac{p}{1-p}=\\frac{1}{5}=20\\%$.\n",
    "\n",
    "The *log-odds* is simply the logarithm of the odds. The *log-odds* is equivalent to $logit(p)$.\n",
    "$$logit(p)=log\\left(\\frac{p}{1-p}\\right)=log\\left(\\frac{P\\{Y=1\\}}{P\\{Y=0\\}}\\right)$$\n",
    "\n",
    "##### Change in $p$\n",
    "\n",
    "Imagine we have\n",
    "$$logit(p) = b_0+b_1x_1+b_2x_2+\\dots$$\n",
    "where $b_1=0.23$. Increasing $x_1$ results in a $0.23$ increase in $logit(p)=log\\left(\\frac{p}{1-p}\\right)$. Therefore, if $log\\left(\\frac{p}{1-p}\\right)$ increases by $0.23$, then $\\frac{p}{1-p}$ increases by $e^{0.23}=1.259$, which is a $25.9\\%$ increase if all variables remain constant/fixed.\n",
    "\n",
    "* Quantitative Interpretation - For every one unit increase in $x_1$, expect a multiplicative change in the odds of a $1$ by $e^{b_1}$.\n",
    "* Categorical Interpretation - When in category $x_1$, we expect a multiplicative change in the odds of a 1 by $e^{b_1}$ compared to the baseline.\n",
    "\n",
    "See [Example 2](Examples\\Regression\\logistic-regression.ipynb) for an implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics & Performance Metrics\n",
    "\n",
    "##### Accuracy\n",
    "\n",
    "Accuracy is the most common metric for determining the success of a model. It is defined as\n",
    "$$\\text{Accuracy} = \\frac{\\text{\\# correct labels}}{\\text{\\# data points}}$$\n",
    "\n",
    "##### Confusion Matrices\n",
    "\n",
    "A confusion matrix shows us the true and predicted labels for all data points in our dataset. Typically, the true labels are shown on the left (i.e., rows) while the predicted labels are shown on top (i.e., columns).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "True values are on the rows and predicted values are on the columns.\n",
    "\n",
    "| | Sharon | Powell | Rumsfeld | Bush | Schroeder | Chavez | Blair |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| **Sharon** | 13 | 4 | 1 | 1 | 0 | 0 | 1 |\n",
    "| **Powell** | 0 | 55 | 0 | 8 | 0 | 0 | 0 |\n",
    "| **Rumsfeld** | 0 | 1 | 25 | 8 | 0 | 0 | 2 |\n",
    "| **Bush** | 0 | 3 | 0 | 123 | 0 | 0 | 1 |\n",
    "| **Schroeder** | 0 | 1 | 0 | 7 | 14 | 0 | 4 |\n",
    "| **Chavez** | 0 | 3 | 0 | 2 | 1 | 10 | 0 |\n",
    "| **Blair** | 0 | 0 | 1 | 7 | 0 | 0 | 26 |\n",
    "\n",
    "From this, we can see\n",
    "* There are $26$ images of Schroeder and $15$ predictions for Schroeder.\n",
    "* RECALL:  Given we have an image of Chavez, our model is likely to predict Chavez $10/16$ or $62.5\\%$ of the time.\n",
    "* PRECISION:  Given an images was classified as Chavez, we should expect that the image is Chavez $10/10$ or $100\\%$ of the time.\n",
    "\n",
    "##### Recall & Precision\n",
    "\n",
    "Recall:  Out of all the items that are truly positive, how many were correctly classified as positive? (How many positive items were 'recalled' from the dataset?)\n",
    "$$\\text{Recall} = \\frac{\\text{True Positive}}{\\text{True Positive + False Negative}}$$\n",
    "\n",
    "Precision:  OUt of all the items labeled as positive, how many truly belong to the positive class?\n",
    "$$\\text{Precision} = \\frac{\\text{True Positive}}{\\text{True Positive + False Positive}}$$\n",
    "\n",
    "Using another example from the above confusion matrix, we can see that\n",
    "* The recall rate for Powell is $55/63$ or $0.873$.\n",
    "* The precision rate for Powell is $55/67$ or $0.821$.\n",
    "* The number of true positives for Rumsfeld is $25$.\n",
    "* The number of false positives for Rumsfeld is $2$.\n",
    "* The number of false negatives for Rumsfeld is $11$.\n",
    "\n",
    "See [Example 3](Examples\\Regression\\logistic-regression.ipynb) for an implementation.\n",
    "\n",
    "##### ROC Curves and AUC\n",
    "\n",
    "Another very popular set of metrics are [ROC curves and AUC](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py).  These actually use the probability from the logistic regression models, and not just the label.  [This blog](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) is also a great resource for understanding ROC curves and AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up - Statistics vs Machine Learning\n",
    "\n",
    "* Statistics\n",
    "    * Determine relationships and understand the driving mechanisms... Are relationships due to chance?  \n",
    "* Machine Learning (Supervised)\n",
    "    * Work to predict as well as possible, often without regard to why it works well."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
