{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Linear Regression\n",
    "\n",
    "When our data is linear, we found that we can describe the correlation between two variables using Person's r and a line drawn through our data. This line can be called the *regression line* or the *line of best fit* and will help us describe our data and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "Once a line of best fine has been determined (by finding the slope and y-intercept), we can make estimates for unseen or future values. Our dataset, $\\mathcal{D}$, consists of all the pairs of observed ($x$,$y$).\n",
    "\n",
    "With the slope, $b$, and y-intercept, $a$, determined, we can make predictions $\\hat{y}=bx+a$. For a known data point ($x$, $y$), the difference between $y$ and $\\hat{y}$ is called the *residual*.\n",
    "\n",
    "When conducting regression, $a$ and $b$ are known as *regression coefficients*. It is important to note that software packages often present the coefficients in $\\hat{y}=a+bx$ order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Line of Best Fit\n",
    "To compute the line of best fit, we imagine an optimization problem that attempts to minimize the difference between our models outputs (our guesses), $\\hat{y}$ and the actual value from our data $y$.\n",
    "\n",
    "##### Minimize Sum of Residuals\n",
    "One way we could think about computing the line of best fit is to minimize the sum of residuals, $$\\underset{a,b}{\\operatorname{\\argmin}}\\sum\\limits_{(x,y)\\in\\mathcal{D}}y-\\hat{y}$$\n",
    "\n",
    "However, we will find that we can have poor lines of fit in this case. All lines in which the sum of positive and negative residuals is nearly equal results in equally good lines of best fit.\n",
    "\n",
    "For this reason, we usually employ an alternative method that either sums the squares or absolute values of residuals.\n",
    "\n",
    "##### Minimize Sum of Absolute Residuals\n",
    "To ensure the values of the residuals is always positive, we can consider summing the absolute value of residuals, $$\\underset{a,b}{\\operatorname{\\argmin}}\\sum\\limits_{(x,y)\\in\\mathcal{D}}\\lvert y-\\hat{y}\\rvert$$\n",
    "\n",
    "As we learned in calculus, we can take the derivative of the expression to identify the minimum. $$\\underset{x}{\\operatorname{\\min}} \\lvert x \\rvert= \\text{sign}(x)$$\n",
    "\n",
    "We find that using the absolute value in our expression results in sparse solutions. In machine learning, using this term in a loss function is known as L1, or Lasso, regularization. While this is an option, we will look to the sum of squared residuals for this course.\n",
    "\n",
    "##### Minimize Sum of Squared Residual\n",
    "Another way to overcome the cancellation of positive and negative values is to square the residuals. Thus, our objective is to minimize this sum of the square of the residuals. $$\\underset{a,b}{\\operatorname{\\argmin}}\\sum\\limits_{(x,y)\\in\\mathcal{D}}(y-\\hat{y})^2$$\n",
    "\n",
    "Using calculus, we can solve for the slope to find\n",
    "\n",
    "$$b = \\underset{b}{\\operatorname{\\argmin}}\\sum\\limits_{(x,y)\\in\\mathcal{D}}(y-\\hat{y})^2 = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}$$\n",
    "\n",
    "Now, we can look ack to our terms for our covariance, standard deviation, $S$, and Person's r, $r$. $$S_x = \\sum(x_i-\\bar{x})$$ $$r=\\frac{\\text{Cov}(x, y)}{S_x\\cdot S_y}$$\n",
    "\n",
    "We can now write $b$ in these terms. $$b=r\\frac{S_x}{S_y}$$\n",
    "\n",
    "Therefore, we can now represent our regression line as $$\\hat{y} = a + bx = a + r\\frac{S_x}{S_y}x$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
