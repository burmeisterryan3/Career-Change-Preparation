{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "With simple linear regression, we had\n",
    "$$\\hat{y} = b_0 + b_1x_1$$\n",
    "\n",
    "With multiple linear regression, we extend simple linear regression using both quantitative and categorical explanatory ($x$) variables to predict a quantitative response.\n",
    "\n",
    "$$\\hat{y} = b_0 + b_1x_1 + b_2x_2 + \\dots + b_mx_m$$"
   ]
  },
  {
   "attachments": {
    "residual-plots.gif": {
     "image/gif": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MLR Assumptions](https://online.stat.psu.edu/stat462/node/145/)\n",
    "\n",
    "The four conditions (\"*LINE*\") that comprise the multiple linear regression model generalize the simple linear regression model conditions to take account that we now have multiple predictors.\n",
    "\n",
    "* The mean of the response, $\\mathrm{E}(Y_i)$, at each set of predictors, $\\mathbf{x}_i = (x_{1i}, x_{2i}, \\dots)$, is a **linear function** of the predictors.\n",
    "    * An equivalent way to think of this condition is that the mean of the error, $\\mathrm{E}(\\varepsilon_i)$, at each set of values of the predictors, $\\mathbf{x}_i = (x_{1i}, x_{2i}, \\dots)$, is **zero**.\n",
    "* The errors, $\\varepsilon_i$, are **independent**.\n",
    "* The errors, $\\varepsilon_i$, at each set of values of the predictors, $\\mathbf{x}_i = (x_{1i}, x_{2i}, \\dots)$, are **normally distributed**.\n",
    "* The errors, $\\varepsilon_i$, at each set of values of the predictors, $\\mathbf{x}_i = (x_{1i}, x_{2i}, \\dots)$, have **equal variance**.\n",
    "\n",
    "An alternative way to describe all four assumptions is that the errors, $\\epsilon_i$, are independent normal random variables with mean zero and constant variance, $\\sigma^2$.\n",
    "\n",
    "As in simple linear regression, we can assess whether these conditions seem to hold for a multiple linear regression model applied to a particular sample dataset by looking at the estimated errors, i.e., the residuals, $e_i = y_i - \\hat{y}_i$.\n",
    "\n",
    "##### What can go wrong?\n",
    "\n",
    "The four conditions of the model tells us what can go wrong with our model, namely:\n",
    "\n",
    "* The population regression function is **not linear**. That is, the response $Y_i$ is not a function of linear trend ($\\beta_0+\\beta_1x_i$) plus some error, $\\varepsilon_i$.\n",
    "* The error terms are **not independent** (i.e., [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) of errors).\n",
    "* The error terms are **not normally distributed**.\n",
    "* The error terms do **not have equal variance** (i.e., [heteroscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity) of errors).\n",
    "\n",
    "Sometimes, these errors can arise due to outliers, [Leverage Points](https://online.stat.psu.edu/stat462/node/170/), or multicollinearity of our predictor variables.\n",
    "\n",
    "![residual-plots.gif](attachment:residual-plots.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 - House Prices\n",
    "\n",
    "See Example 1 in [multiple-regression.ipynb](Examples\\Regression\\multiple-regression.ipynb) for an multiple linear regression example using statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price  \\\n",
       "0      1112            B  1188         3          2      ranch   598291   \n",
       "1       491            B  3512         5          3  victorian  1744259   \n",
       "2      5952            B  1134         3          2      ranch   571669   \n",
       "3      3525            A  1940         4          2      ranch   493675   \n",
       "4      5108            B  2208         6          4  victorian  1101539   \n",
       "\n",
       "   intercept  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.read_csv('./Data/house_prices.csv')\n",
    "\n",
    "# Add intercept column\n",
    "df['intercept'] = 1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Solution to Multiple Linear Regression\n",
    "\n",
    "Credit: [Criteria for Estimates](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)\n",
    "\n",
    "Our *estimates* f the population parameters are referred to as $\\hat{\\boldsymbol{\\beta}}$. Recall that the criteria we use for obtaining our estimates is to find the estimator $\\hat{\\beta}$ that minimizes the sum of squared residuals ($\\sum{e_i^2}$ in scalar notation).\n",
    "\n",
    "The vector of residuals $e$ is given by:\n",
    "$$\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$$\n",
    "\n",
    "The sum of squared residuals (RSS) is $\\mathbf{e}^Te$.\n",
    "$$\\begin{bmatrix}e_1&e_2&\\dots&\\dots&e_n\\end{bmatrix}_{1\\times n}\\begin{bmatrix}e_1\\\\ e_2\\\\\\vdots\\\\\\vdots\\\\ e_n\\end{bmatrix}_{n\\times1} = \\begin{bmatrix}e_1\\times e_1 + e_2\\times e_2 + \\dots + e_n\\times e_n\\end{bmatrix}_{1\\times1}$$\n",
    "\n",
    "Thus, we can write\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        e^Te\\;=&\\;(\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) \\\\\n",
    "        =&\\;\\mathbf{y}^T\\mathbf{y}-\\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}+\\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\\\\n",
    "        =&\\;\\mathbf{y}^T\\mathbf{y} - 2\\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\mathbf{y}+\\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where we use the fact that the transpose of a scalar is the scalar, i.e., $\\mathbf{y}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}=(\\mathbf{y}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T=\\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\mathbf{y}$.\n",
    "\n",
    "To find the $\\hat{\\boldsymbol{\\beta}}$ that minimizes the RSS, we take the derivative of our previous equation with respect to $\\hat{\\boldsymbol{\\beta}}$ and set it equal to 0.\n",
    "$$\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial\\hat{\\boldsymbol{\\beta}}} = -2\\mathbf{X}^T\\mathbf{y}+\\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}=0$$\n",
    "\n",
    "To check that this is a minimum, we would take the derivative with respect to $\\hat{\\boldsymbol{\\beta}}$ again, which gives us $2\\mathbf{X}^T\\mathbf{X}$. As long as $\\mathbf{X}$ has full rank, this is a positive definite matrix (analogous to a positive real number) and hence a minimum.\n",
    "\n",
    "From the above equation, we get the *normal equation*.\n",
    "$$(\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}}=\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "First, we should note $(\\mathbf{X}^T\\mathbf{X})$ is always square, e.g., $k\\times k$. Second, it is always symmetric.\n",
    "\n",
    "Recall that $(\\mathbf{X}^T\\mathbf{X})$ and $\\mathbf{X}^T\\mathbf{y}$ are known from our data but $\\hat{\\boldsymbol{\\beta}}$ is unknown. If the inverse, $(\\mathbf{X}^T\\mathbf{X})^{-1}$, exists, then we have\n",
    "$$\n",
    "(\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "$$\n",
    "\n",
    "where, by definition, $(\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}}=\\mathbf{I}$ is a $k\\times k$ identity matrix.\n",
    "\n",
    "This gives us\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{I}\\hat{\\boldsymbol{\\beta}}&\\; =& (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
    "    \\hat{\\boldsymbol{\\beta}}&\\; =& (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since the estimators in the $\\hat{\\boldsymbol{\\beta}}$ vector are a linear combination of random variables, they themselves are random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the Coefficient Using the Closed Form Solution\n",
    "\n",
    "Now that we now $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$, let's try to compute the estimators (or coefficients) ourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10072.10704671,  7345.3917137 , -2925.80632467,   345.91101884])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['intercept', 'bathrooms', 'bedrooms', 'area']]\n",
    "y = df['price']\n",
    "\n",
    "# Compute closed form solution\n",
    "beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding our Results\n",
    "\n",
    "Each coefficient can be understood as the predicted increase or decrease in the response variable for every one unit increase in the explanatory variable, holding all other variables in the model constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Categorical Variables\n",
    "\n",
    "##### 0,1 Coding Scheme\n",
    "\n",
    "The way we add categorical variables into our multiple linear regression model is by using dummy variables. The most common way dummy variables are added is through $1,0$ encoding. In this method, you create a new column for each category of the categorical variable with the value being $1$ or $0$ depending on whether the value was present in the original column.\n",
    "\n",
    " To ensure we can invert our matrix, we must keep our columns independent making our matrix [full rank](https://www.cds.caltech.edu/~murray/courses/cds101/fa02/faq/02-10-28_fullrank.html). When do this dummy variables by always dropping one of the columns given that we can infer it if we have each of the other columns and would create collinear columns in our matrix.\n",
    " \n",
    " For instance, if a particular value has not been present in the previous $k-1$ categories, then it will be present in the $k\\text{th}$ category. The column you drop is called the **baseline**. The coefficients obtained from the output of a multiple linear regression model are then an indication of how the encoded categories compare to the baseline category.\n",
    "\n",
    "| Neighborhood |\n",
    "| --- |\n",
    "| A |\n",
    "| B |\n",
    "| C |\n",
    "| B |\n",
    "| A |\n",
    "| A |\n",
    "| A |\n",
    "| C |\n",
    "\n",
    "would become\n",
    "\n",
    "| A | B |\n",
    "| --- | --- |\n",
    "| 1 | 0 |\n",
    "| 0 | 1 |\n",
    "| 0 | 0 |\n",
    "| 0 | 1 |\n",
    "| 1 | 0 |\n",
    "| 1 | 0 |\n",
    "| 1 | 0 |\n",
    "| 0 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2 - Dummy Variables\n",
    "\n",
    "See Example 2 in [multiple-regression.ipynb](Examples\\Regression\\multiple-regression.ipynb) for an multiple linear regression example using dummy variables and statsmodels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -1,0,1 Coding Scheme\n",
    "\n",
    "In some software (e.g. SAS), a -1,0,1-coding scheme is used in lieu of a 0,1-coding scheme. In the 0,1-coding scheme, comparisons are made to the baseline category (the one left out). In the -1,0,1-coding scheme, comparisons are made to the overall average.\n",
    "\n",
    "In order to predict the baseline category in the 1,0 coding, ou use the intercept. In the 1,0,-1 coding scheme, you need to multiple each categorical coefficient by -1 to get the missing category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Assessing the Model Assumptions](https://online.stat.psu.edu/stat462/node/146/)\n",
    "\n",
    "We can use the same types of test as in the case of [simple linear regression](11.A.%20Simple%20Linear%20Regression.ipynb). Other tests include:\n",
    "\n",
    "* [Test for Error Normality](https://online.stat.psu.edu/stat462/node/147/)\n",
    "* [Tests for Constant Error Variance](https://online.stat.psu.edu/stat462/node/148/)\n",
    "* Tests for Correlated Errors\n",
    "    * [Durbin-Watson](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic)\n",
    "    * [ARIMA or ARMA](http://www.statsref.com/HTML/index.html?arima.html)\n",
    "    * Bivariate Plots or [Variance Inflation Factors](https://en.wikipedia.org/wiki/Variance_inflation_factor) (VIFs)\n",
    "        * See [Example 4](Examples\\Regression\\multiple-regression.ipynb) for a [VIF example](https://etav.github.io/python/vif_factor_python.html).\n",
    "\n",
    "##### Multicollinearity\n",
    "\n",
    "When building a multiple linear regression model, we want our x-variables (explanatory/predictor) variables to be related to the response but not to one another. When our x-variables are correlated with one another, this is known as **multicollinearity**. Multicollinearity has two potential negative impacts.\n",
    "\n",
    "* The expected relationships between the explanatory and response variables do not hold\n",
    "    * For instance, you expect a positive relationship between an explanatory variable and the response (based on the bivariate relationships), but in the multiple linear regression case, the relationship is negative.\n",
    "* Our hypothesis testing is not reliable.\n",
    "    * Having correlated explanatory variables means that our coefficient estimates are less stable - the standard deviations (or standard errors) associated with the coefficients are large. Therefore, an association for particular variable that might be useful for predicting a response is no longer seen.\n",
    "\n",
    "We can identify multicollinearity a couple of ways.\n",
    "\n",
    "* Look at the correlation of each explanatory variable with all the other explanatory variables.\n",
    "    * Scatter plot\n",
    "    * Correlation coefficients differences from the bivariate relationships\n",
    "* Look at the VIFs for each variable.\n",
    "    * VIFs greater than 10 suggest multicollinearity. (Some experts suggest dropping that value even to 5.)\n",
    "\n",
    "The most common way of working with correlated explanatory variables is simply to remove one of the variables that is most related to the other variables and of least interest to your model.\n",
    "\n",
    "##### Variance Inflation Factor Computation\n",
    "$$VIF_i = \\frac{1}{1-R_i^2}$$\n",
    "\n",
    "In the event our x-variables are correlated, then we should be able to predict its value based on the remaining x-variables in our model.\n",
    "\n",
    "$$x_i = b_0 + b_1x_1 + b_2x_2 + \\dots + b_nx_n$$\n",
    "\n",
    "All other x-variables - excluding $x_i$ - are then used to make predict then compute $R_i^2$, where $R_i^2$ is higher when $x_i$ is correlated with other variables (just as in the case with simple linear regression between $x$ and $y$).\n",
    "\n",
    "Therefore, as $R_i^2$ increases our denominator decreases which, in turn, increases $VIF_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Order Terms\n",
    "\n",
    "When running MLR, it is possible to use higher order terms, such as $x_1x_2$, $x^2$, $x^3$, $x^4$, etc.\n",
    "\n",
    "It is generally recommended to avoid using these terms in favor of the explanability of our simpler terms, e..g., slope of the sum of square footage of a home, unless we have good reason for their inclusion. This is primarily due to the fact that these terms become more difficult to interpret when the higher order terms show up.\n",
    "\n",
    "If we do end up adding higher order terms to our model, we should be sure to include the lower order terms as well (e.g., $x_1$ if $x_1x_2$ or $x_1^2$ is included)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to Identify Higher Order Terms? Quadratic & Cubic Terms\n",
    "\n",
    "Higher order terms in linear models are created when multiplying two or more x-variables by one another. Common higher order terms include quadratics (e.g., $x_i^2$), cubics (e.g., $x_i^3$), or interactions (e.g., two or more x-variables are multiplied by each other, $x_1x_2$).\n",
    "\n",
    "Previously, we had\n",
    "$${\\hat{y} = b_0 + b_1x_1 + b_2x_2.}$$\n",
    "\n",
    "If we decide the linear model can be improved with higher order terms, the equation might change to\n",
    "$${\\hat{y} = b_0 + b_1x_1 + b_2x_1^2 + b_3x_2 + b_4x_1x_2.}$$\n",
    "\n",
    "When creating models with *quadratic*, *cubic*, or even higher orders of a variable, we are essentially looking at how many curves there are in the relationships between the explanatory and response variables.\n",
    "\n",
    "If there is one curve, like in a quadratic plot. Then we will want to add a quadratic term, as clearly a line won't be the best fit for this relationship.\n",
    "\n",
    "<div align=\"center\"><img src=\"./Images/quadraticlinearregression.png\" width=\"600\"/></div>\n",
    "\n",
    "If there are two curves, then we may consider a cubic relationship.\n",
    "\n",
    "<div align=\"center\"><img src=\"./Images/cubiclinearregression.jpg\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to Identify Higher Order Terms? Interaction Terms\n",
    "\n",
    "We add interaction terms, e.g., $x_1x_2$, when the way that an explanatory variable, e.g., $x_1$, is related to your response variable is dependent on the value of another explanatory variable, e.g., $x_2$.\n",
    "\n",
    "Using price, area, and neighborhood as an example, we may find that the slope of a line of best fit for the prices maintains the same slope for two different neighborhoods, though the price differs by a (roughly) constant amount for each. In the event this is true, we do *not* need an interaction term.\n",
    "\n",
    "Here, $b_1$ is the way we estimate the relationship between *area* and *price*, which is the model we believe to be the same regardless of the neighborhood. Then, $b_2$ is the difference in price depending on which neighborhood you are in, which is the vertical distance between the two lines.\n",
    "\n",
    "<div align=\"center\"><img src=\"./Images/higher-order-no-interaction-term.png\" width=\"600\"></div>\n",
    "\n",
    "From this we can see that\n",
    "* The way that *area* is related to *price* is the same regardless of the *neighborhood* (i.e., same slope for both lines).\n",
    "* The difference in *price* for different *neighborhoods* is the same regardless of the *area*.\n",
    "\n",
    "When both of the above are true, we do not need an interaction term. However, if we find the way that *area* is related to *price* is different depending on the *neighborhood*, then we need an interaction term.\n",
    "\n",
    "By adding the interaction, we allow the slopes of line for each neighborhood to be different. These lines might even cross or grow apart quickly. Either of these would suggest an interaction is present between *area* and *neighborhood* in the way they relate to the *price*.\n",
    "\n",
    "<div align=\"center\"><img src=\"./Images/higher-order-interaction-term.png\" width=\"600\"></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
