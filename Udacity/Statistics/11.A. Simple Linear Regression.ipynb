{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "### Machine Learning Introduction\n",
    "\n",
    "**Machine Learning** is frequently split into **supervised** and **unsupervised** learning. Regression, which you will be learning about in this lesson (and its extensions in later lessons), is an example of supervised machine learning.\n",
    "\n",
    "In supervised machine learning, you are interested in predicting a label for your data. Commonly, you might want to predict fraud, customers that will buy a product, or home values in an area.\n",
    "\n",
    "In unsupervised machine learning, you are interested in clustering data together that isn't already labeled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots\n",
    "\n",
    "Scatter plots are a common visual for comparing two quantitative variables. A common summary statistic that relates to a scatter plot is the **correlation coefficient** commonly denoted by r.\n",
    "\n",
    "Though there are a [few different ways](http://www.statisticssolutions.com/correlation-pearson-kendall-spearman/) to measure correlation between two variables, the most common way is with [Pearson's correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient). Pearson's correlation coefficient provides the:\n",
    "\n",
    "1. Strength\n",
    "1. Direction\n",
    "\n",
    "of a **linear relationship**. [Spearman's Correlation Coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) does not measure linear relationships specifically, and it might be more appropriate for certain cases of associating two variables.\n",
    "\n",
    "##### Correlation Coefficients\n",
    "\n",
    "Correlation Coefficients\n",
    "Correlation coefficients provide a measure of the strength and direction of a linear relationship.\n",
    "\n",
    "We can tell the direction based on whether the correlation is positive or negative.\n",
    "\n",
    "A general rule of thumb for judging the strength:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\text{Strong}\\quad &\\text{Moderate} \\quad&\\text{Weak} \\\\\n",
    "&0.7\\leq\\vert r\\vert\\leq 1.0 \\quad&0.3\\leq\\vert r\\vert\\lt 0.7\\quad&0.0\\leq\\vert r\\vert\\lt 0.3\n",
    "\\end{aligned}$$\n",
    "\n",
    "##### Calculation of the Correlation Coefficient\n",
    "\n",
    "$$r=\\frac{\\text{Cov}(X, Y)}{S_X\\cdot S_Y} = \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum(x_i-\\bar{x})^2}\\sqrt{\\sum(y_i-\\bar{y})^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "In simple linear regression, we compare two quantitative variables to one another.\n",
    "\n",
    "The **response** (or dependent) variable is what you want to predict, while the **explanatory** (or independent) variable is the variable you use to predict the response. A common way to visualize the relationship between two variables in linear regression is using a scatter plot.\n",
    "\n",
    "##### Defining a Line\n",
    "\n",
    "A line is commonly identified by an **intercept** and a **slope**.\n",
    "\n",
    "The **intercept** is defined as the **predicted value of the response when the x-variable is zero**.\n",
    "\n",
    "The **slope** is identified as the **predicted change in the response variable for every one unit increase in the x-variable**.\n",
    "\n",
    "We notate the line in linear regression as:\n",
    "$$\\hat{y} = b_0 + b_1x_1$$\n",
    "where\n",
    "* $\\hat{y}$ is the predicted value of the response from the line, \n",
    "* $b_0$ is the intercept (determined from our sample),\n",
    "* $b_1$ is the slope (determined from our sample),\n",
    "* $x_1$ is the explanatory variable, and\n",
    "* $y$ is the actual response for a a data point in our data set (i.e., not our prediction from our line).\n",
    "\n",
    "The actual (population) parameters are annotated as\n",
    "* $\\beta_0$ for the intercept and\n",
    "* $\\beta_1$ for the slope.\n",
    "\n",
    "##### Fitting a Regression Line\n",
    "\n",
    "The main algorithm used to find the best fit line is called the **least-squares** algorithm, which finds the line that minimizes $\\sum\\limits_{i=1}^n(y_i-\\hat{y}_i)^2$.\n",
    "\n",
    "There are many other ways to choose a \"best\" line, but this algorithm tends to do a good job in many scenarios.\n",
    "\n",
    "In order to compute the slope and intercept we would need to compute the following.\n",
    "\n",
    "$$\\begin{align}\\bar{x}=&\\frac{1}{n}\\sum x_i \\\\\\bar{y}=&\\sum y_i\\\\ s_y =&\\sqrt{\\frac{1}{n-1}\\sum{(y_i-\\bar{y})^2}}\\\\ s_x=&\\sqrt{\\frac{1}{n-1}\\sum{(x_i-\\bar{x})^2}}\\\\ r=&\\frac{\\sum\\limits_{i=1}^{n}{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum{(x_i-\\bar{x})^2}}\\sqrt{\\sum{(y_i-\\bar{y})^2}}}\\end{align}$$\n",
    "\n",
    "Taking the derivative of our least squared equations, we find that\n",
    "$$\\begin{align}b_1&=r\\frac{s_x}{s_y}\\\\ b_0&=\\bar{y}-b_1\\bar{x}\\end{align}$$\n",
    "\n",
    "##### Example using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>598291</td>\n",
       "      <td>1188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1744259</td>\n",
       "      <td>3512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>571669</td>\n",
       "      <td>1134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>493675</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1101539</td>\n",
       "      <td>2208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    price  area\n",
       "0           0   598291  1188\n",
       "1           1  1744259  3512\n",
       "2           2   571669  1134\n",
       "3           3   493675  1940\n",
       "4           4  1101539  2208"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./Data/house_price_area_only.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>598291</td>\n",
       "      <td>1188</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1744259</td>\n",
       "      <td>3512</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>571669</td>\n",
       "      <td>1134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>493675</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1101539</td>\n",
       "      <td>2208</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    price  area  intercept\n",
       "0           0   598291  1188          1\n",
       "1           1  1744259  3512          1\n",
       "2           2   571669  1134          1\n",
       "3           3   493675  1940          1\n",
       "4           4  1101539  2208          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.269e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 12 Jan 2024</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:17:23</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 9587.8878</td> <td> 7637.479</td> <td>    1.255</td> <td> 0.209</td> <td>-5384.303</td> <td> 2.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  348.4664</td> <td>    3.093</td> <td>  112.662</td> <td> 0.000</td> <td>  342.403</td> <td>  354.530</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>368.609</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 349.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.534</td>  <th>  Prob(JB):          </th> <td>1.43e-76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.499</td>  <th>  Cond. No.          </th> <td>4.93e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.93e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                 1.269e+04\n",
       "Date:                Fri, 12 Jan 2024   Prob (F-statistic):               0.00\n",
       "Time:                        14:17:23   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6026   BIC:                         1.691e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   9587.8878   7637.479      1.255      0.209   -5384.303    2.46e+04\n",
       "area         348.4664      3.093    112.662      0.000     342.403     354.530\n",
       "==============================================================================\n",
       "Omnibus:                      368.609   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              349.279\n",
       "Skew:                           0.534   Prob(JB):                     1.43e-76\n",
       "Kurtosis:                       2.499   Cond. No.                     4.93e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.93e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = sm.OLS(df['price'], df[['intercept', 'area']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding the Results - Coefficients\n",
    "\n",
    "We can use the above table to help us determine if there is a statistically significant linear relationship between a particular variable and the response. (Note: The hypothesis test ofr the intercept isn't useful in most cases.)\n",
    "\n",
    "However, the hypothesis test for each x-variable is a test of whether the population slope is equal to zero compared to an alternative where the parameter differs from zero.\n",
    "\n",
    "$$\\begin{align}\n",
    "H_0:&\\quad\\beta_1 = 0\\\\\n",
    "H_A:&\\quad\\beta_1 \\neq 0\n",
    "\\end{align}$$\n",
    "\n",
    "In the example above, we see that the area has a p-value of $0$, meaning it is statistically significant when predicting a response and that there is statistical evidence that the population slope associated with area relating to price is non-zero. Therefore, if the slope is different than zero (the reject $H_0$), we have evidence that the x-variable attached to that coefficient has a statistically significant linear relationship with the response, which suggests it should help us in predicting the response.\n",
    "\n",
    "Our results suggest $$\\hat{y}=\\beta_0+\\beta_1x=9588+348.5x$$\n",
    "\n",
    "##### Understanding the Results - R-squared\n",
    "\n",
    "R-squared is the square of (Pearson's) correlation coefficient. It is the amount of variability in the response (i.e., $y$) explained by your model. The closer this value is to 1, the better our model fits the data.\n",
    "\n",
    "In our example, the R-squared value suggests 67.8% of the price of the homes can be explained by the area of the home.\n",
    "\n",
    "[Here](http://data.library.virginia.edu/is-r-squared-useless/) you can find one argument explaining why one individual (a professor at Carnegie Mellon!) doesn't care for R-squared. However, cross-validation can assist us with validating any measure that helps us understand the fit of a model to our data."
   ]
  },
  {
   "attachments": {
    "residual-plots.gif": {
     "image/gif": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Simple Linear Regression Assumptions](https://online.stat.psu.edu/stat462/node/116/)\n",
    "\n",
    "The four conditions (\"*LINE*\") that comprise the multiple linear regression model generalize the simple linear regression model conditions to take account that we now have multiple predictors.\n",
    "\n",
    "* The mean of the response, $\\mathrm{E}(Y_i)$, at each value of the predictor, $x_i$, is a **linear function** of the predictors.\n",
    "    * An equivalent way to think of this condition is that the mean of the error, $\\mathrm{E}(\\varepsilon_i)$, at each value of the predictor, $x_i$, is **zero**.\n",
    "* The errors, $\\varepsilon_i$, are **independent**.\n",
    "* The errors, $\\varepsilon_i$, at each value of the predictors, $x_i$, are **normally distributed**.\n",
    "* The errors, $\\varepsilon_i$, at each value of the predictors, $x_i$, have **equal variance** (denoted $\\sigma^2$).\n",
    "\n",
    "An alternative way to describe all four assumptions is that the errors, $\\varepsilon_i$, are independent normal random variables with mean zero and constant variance, $\\sigma^2$.\n",
    "\n",
    "As in simple linear regression, we can assess whether these conditions seem to hold for a multiple linear regression model applied to a particular sample dataset by looking at the estimated errors, i.e., the residuals, $e_i = y_i - \\hat{y}_i$.\n",
    "\n",
    "##### What can go wrong?\n",
    "\n",
    "The four conditions of the model tells us what can go wrong with our model, namely:\n",
    "\n",
    "* The population regression function is **not linear**. That is, the response $Y_i$ is not a function of linear trend ($\\beta_0+\\beta_1x_i$) plus some error, $\\epsilon_i$.\n",
    "* The error terms are **not independent** (i.e., [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) of errors).\n",
    "* The error terms are **not normally distributed**.\n",
    "* The error terms do **not have equal variance** (i.e., [heteroscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity) of errors).\n",
    "\n",
    "(![residual-plots.gif](attachment:residual-plots.gif))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Assessing the Model Assumptions](https://online.stat.psu.edu/stat462/node/146/)\n",
    "\n",
    "We can use the same types of test as in the case of [simple linear regression](11.A.%20Simple%20Linear%20Regression.ipynb). Other tests include:\n",
    "\n",
    "* [Test for Error Normality](https://online.stat.psu.edu/stat462/node/147/)\n",
    "* [Tests for Constant Error Variance](https://online.stat.psu.edu/stat462/node/148/)\n",
    "* Tests for Correlated Errors\n",
    "    * [Durbin-Watson](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic)\n",
    "    * [ARIMA or ARMA](http://www.statsref.com/HTML/index.html?arima.html)\n",
    "    * Bivariate Plots or [Variance Inflation Factors](https://en.wikipedia.org/wiki/Variance_inflation_factor) (VIFs)\n",
    "        * See [Example 4](Examples\\Regression\\multiple-regression.ipynb) for a [VIF example](https://etav.github.io/python/vif_factor_python.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Residual Analysis\n",
    "\n",
    "The vertical distance between any one data point $y_i$ and its estimated value $\\hat{y}_i$ is its observed **residual**.\n",
    "$$e_i = y_i - \\hat{y}_i$$\n",
    "\n",
    "Each observed residual can be thought of as an estimate of the actual unknown \"true error\" term.\n",
    "$$\\varepsilon_i = Y_i - \\mathrm{E}(Y_i)$$\n",
    "\n",
    "The observed residuals should reflect the properties assumed for the unknown error terms. The basic idea of residual analysis is to investigate the observed residuals to see if they support the assumptions of linearity, independence, normality, and equal variances.\n",
    "\n",
    "Here are a few plots that can support the analysis.\n",
    "* [Residuals vs Fits Plot](https://online.stat.psu.edu/stat462/node/117/) or [Residuals vs Predictor Plot](https://online.stat.psu.edu/stat462/node/118/)\n",
    "    * How does a non-linear regression function show up on a residual vs fits plot?\n",
    "        * The residuals depart from $0$ in some *systematic manner*, such as being positive for small $x$ values, negative for medium $x$ values, and positive again for large $x$ values. Any systematic pattern is sufficient to suggest that the regression function is not linear.\n",
    "    * How does non-constant error variance show up on a residual vs fits plot?\n",
    "        * The plot has a \"**fanning**\" effect. That is, the residuals are close to $0$ for small $x$ values and are more spread out for large $x$ values.\n",
    "        * The plot has a \"**funneling**\" effect. That is, the residuals are spread out for small $x$ values and close to $0$ for large $x$ values.\n",
    "        * The spread of residuals varies in a systematic but complex fashion.\n",
    "    * How does an outlier show up on a residuals vs fits plot?\n",
    "        * The observation's residual stands apart from the basic random pattern of the rest of the residuals. The random pattern of the residual plot can even disappear of one outlier deviates significantly from the patter of the rest of the data.\n",
    "* [Residuals vs Order Plot](https://online.stat.psu.edu/stat462/node/121/)\n",
    "    * Appropriate if we know the order in which the data was collected.\n",
    "* [Normal Probability Plot of Residuals](https://online.stat.psu.edu/stat462/node/122/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* Simple linear regression is about building a line that models the relationship between two quantitative variables.\n",
    "* Correlation coefficients are a measure that can inform you about the strength and direction of a linear relationship.\n",
    "* The most common way to visualize simple linear regression is using a scatterplot.\n",
    "* A line is defined by an intercept and slope, which you found using the statsmodels library in Python.\n",
    "* The intercept is the value of the response variable when the explanatory variable is set to $0$.\n",
    "* The slope is the change in the response variable for one unit change in the explanatory variable.\n",
    "* R-squared is the amount of variability in the response explained by the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
