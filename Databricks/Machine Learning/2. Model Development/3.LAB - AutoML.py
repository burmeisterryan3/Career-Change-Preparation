# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.16.3
# ---

#
# <div style="text-align: center; line-height: 0; padding-top: 9px;">
#   <img src="https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png" alt="Databricks Learning" style="width: 600px">
# </div>

#
# # LAB - AutoML
#
# Welcome to the AutoML Lab! In this lab, you will explore the capabilities of AutoML using the Databricks AutoMl UI and AutoML API. 
#
#
# **Lab Outline:**
#
# In this lab, you will need to complete the following tasks;
#
# * **Task 1 :** Load data set
#
# * **Task 2 :** Create a classification experiemnt using the AutoMl UI.
#
# * **Task 3 :** Create a classification experiment with the AutoML API
#
# * **Task 4 :** Retrieve the best run and show the model URI
#
# * **Task 5 :** Import the notebook for a run
#
#

# ## Requirements
#
# Please review the following requirements before starting the lesson:
#
# * To run this notebook, you need to use one of the following Databricks runtime(s): **13.3.x-cpu-ml-scala2.12**

#
# ## Classroom Setup
#
# Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:

# %run ../Includes/Classroom-Setup-03.LAB

# **Other Conventions:**
#
# Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:

print(f"Username:          {DA.username}")
print(f"Catalog Name:      {DA.catalog_name}")
print(f"Schema Name:       {DA.schema_name}")
print(f"Working Directory: {DA.paths.working_dir}")
print(f"User DB Location:  {DA.paths.datasets}")

# ## Task 1 : Load data set
#
# Load the dataset that will be used for the AutoML experiment.
# + Load the dataset where the table name is `bank_loan`.
# + Display the dataset.

loan_data = spark.sql("SELECT * FROM bank_loan")
display(loan_data)

# ## Task 2: Create Classification Experiment Using AutoML UI
#
# Follow these steps to create an AutoML experiment using the  UI:
#
#   ***Step 1.*** Navigate to the **Experiments** section.
#
#   ***Step 2.*** Click on **Create AutoML Experiment** in the top-right corner.
#
#   ***Step 3.*** Choose a cluster for experiment execution.
#
#   ***Step 4.*** For the ML problem type, select **`Classification`**.
#
#   ***Step 5.*** Select the input training dataset as **`catalog > schema > bank_loan`**.
#
#   ***Step 6.*** Specify **`Personal_Loan`** as the prediction target.
#
#   ***Step 7.*** Deselect the **`ID`**, **`ZIP_Code`** field as it's not needed as a feature.
#
#   ***Step 8.*** In the **Advanced Configuration** section, set the **Timeout** to **5 minutes**.
#
#   ***Step 9.*** Enter a name for your experiment, like `Bank_Loan_Prediction_AutoML_Experiment`.
#

# ## Task 3: Create a Classification Experiment with the AutoML API
#
# Utilize the AutoML API to set up and run a classification experiment. Follow these steps:
#
# 1. **Setting up the Experiment:**
#    - **Specify the Dataset:** Specify the dataset using the Spark table name, which is `bank_loan`.
#    - **Set Target Column:** Assign the target_col to the column you want to predict, which is `Personal_Loan`.
#    - **Adjust Exclude Columns:** Provide a list of columns to exclude from the modeling process after reviewing the displayed dataset.
#    - **Set Timeout Duration:** Determine the timeout_minutes for the AutoML experiment. such as `5` minutes   
#
# 2. **Running AutoML:**
#    - Use the AutoML API to explore various machine learning models.
#
#

# +
from databricks import automl
from datetime import datetime

summary = automl.classify(
    dataset = spark.table("bank_loan"),
    target_col = "Personal_Loan",
    exclude_cols= ["ZIP_Code", "ID"],  # Exclude columns as needed
    experiment_name="Bank_Loan_Prediction_AutoML_Experiment_2",
    timeout_minutes = 5
    ) 
# -

# ## Task 4: Retrieve the best run and show the model URI
#
# Identify the best model generated by AutoML based on a chosen metric. Retrieve information about the best run, including the model URI, to further explore and analyze the model.
#  + Find the experiment id associated with your AutoML run experiment. 
#  + Define a search term to filter for runs. Adjust the search term based on the desired status, such as `FINISHED` or `ACTIVE`. 
#  + Specify the run view type to view only active runs or to view all runs.
#  + Provide the metric you want to use for ordering  and Specify whether you want to order the runs in descending or ascending order.

# +
import mlflow
from mlflow.entities import ViewType

# Use this to find the experiment id if multiple experiments need to be sifted through
# automl_experiment_id = mlflow.search_experiments(
#   filter_string=f"name LIKE '{summary.experiment.name}'",
#   max_results=1,
#   order_by=["last_update_time DESC"])[0].experiment_id

# Find the best run ...
automl_runs_pd = mlflow.search_runs(
  experiment_ids=[summary.experiment.experiment_id],
  filter_string=f"attributes.status='FINISHED'",
  run_view_type=ViewType.ACTIVE_ONLY,
  order_by=["metrics.val_f1_score DESC"]
)
automl_runs_pd.head(1)['run_id'].values[0]
# -

# Print information about the best trial
print(summary.best_trial)

#
# ## Task 5: Import Notebook for a Run
#
# AutoML automatically generates the best run's notebook and makes it available for you. If you want to access to other runs' notebooks, you need to import them.
#
# In this task, you will import the **5th run's notebook** to the **`destination_path`**. 
#
# Show the `url` and `path` of the imported notebook.

automl_best_run = mlflow.get_run(automl_runs_pd.iloc[0].run_id)
# automl_best_run
automl_best_run.data.tags['mlflow.databricks.notebookPath']

# +
destination_path = f"/Users/{DA.username}/imported_notebooks/lab.3-{datetime.now().strftime('%Y%m%d%H%M%S')}"

# Get the path and url for the generated notebook
result = automl.import_notebook(f"runs:/{automl_best_run.info.run_id}/notebooks/training_notebook.ipynb", destination_path)

# NOTE: This failed, but this notebook can be found using Databricks UI as shown above
# result = automl.import_notebook(automl_best_run.data.tags['mlflow.databricks.notebookPath'], destination_path)

print(result.path)
print(result.url)
# -

#
# ## Clean up Classroom
#
# Run the following cell to remove lessons-specific assets created during this lesson.

DA.cleanup()

#
# ## Conclusion
#
# In this lab, you got hands-on with Databricks AutoML. You started by loading a dataset and creating a classification experiment using the AutoMl UI and AutoML API. You then learned how to summarize the best model by applying specific filters and explored the process of retrieving the best model along with its Model URI.

# &copy; 2024 Databricks, Inc. All rights reserved.<br/>
# Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href="https://www.apache.org/">Apache Software Foundation</a>.<br/>
# <br/>
# <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a> | <a href="https://help.databricks.com/">Support</a>
